# üìà Horizontal Pod Autoscaler - AI Multi-Model Management System
# Enterprise-grade auto-scaling configuration for production workloads

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-multimodel-api-hpa
  namespace: production
  labels:
    app: ai-multimodel-api
    tier: backend
    environment: production
    app.kubernetes.io/name: ai-multimodel-system
    app.kubernetes.io/component: autoscaler
  annotations:
    description: "HPA for AI Multi-Model API - scales based on CPU, memory, and custom metrics"
    contact: "ai-team@genspark.ai"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-multimodel-api
  
  # üéØ Scaling Configuration
  minReplicas: 5
  maxReplicas: 50
  
  # üìä Scaling Metrics
  metrics:
  
  # üíæ CPU-based scaling
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  
  # üß† Memory-based scaling
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  
  # üåê Request-based scaling (custom metric)
  - type: Pods
    pods:
      metric:
        name: requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  
  # üîÑ Response time-based scaling (custom metric)
  - type: Object
    object:
      metric:
        name: response_time_95th_percentile
      target:
        type: Value
        value: "500m"  # 500ms
      describedObject:
        apiVersion: v1
        kind: Service
        name: ai-multimodel-api
  
  # üö® Error rate-based scaling (custom metric)
  - type: Object
    object:
      metric:
        name: error_rate_percentage
      target:
        type: Value
        value: "2"  # 2% error rate threshold
      describedObject:
        apiVersion: v1
        kind: Service
        name: ai-multimodel-api
  
  # ‚ö° Scaling Behavior Configuration
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60    # Wait 1 minute before scaling up again
      policies:
      - type: Percent
        value: 50                       # Scale up by 50% of current replicas
        periodSeconds: 60
      - type: Pods
        value: 5                        # Or add 5 pods maximum
        periodSeconds: 60
      selectPolicy: Max                 # Use the more aggressive policy
    
    scaleDown:
      stabilizationWindowSeconds: 300   # Wait 5 minutes before scaling down
      policies:
      - type: Percent
        value: 10                       # Scale down by 10% of current replicas
        periodSeconds: 60
      - type: Pods
        value: 2                        # Or remove 2 pods maximum
        periodSeconds: 60
      selectPolicy: Min                 # Use the more conservative policy

---
# üåê Frontend HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-multimodel-frontend-hpa
  namespace: production
  labels:
    app: ai-multimodel-frontend
    tier: frontend
    environment: production
    app.kubernetes.io/name: ai-multimodel-system
    app.kubernetes.io/component: frontend-autoscaler
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-multimodel-frontend
  
  minReplicas: 3
  maxReplicas: 15
  
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 70
  
  # üåê Network traffic-based scaling
  - type: Pods
    pods:
      metric:
        name: nginx_connections_active
      target:
        type: AverageValue
        averageValue: "50"
  
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 3
        periodSeconds: 60
      selectPolicy: Max
    
    scaleDown:
      stabilizationWindowSeconds: 180
      policies:
      - type: Percent
        value: 25
        periodSeconds: 60
      - type: Pods
        value: 1
        periodSeconds: 60
      selectPolicy: Min

---
# üìä Vertical Pod Autoscaler (VPA) for Resource Optimization
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: ai-multimodel-api-vpa
  namespace: production
  labels:
    app: ai-multimodel-api
    tier: backend
    environment: production
    app.kubernetes.io/name: ai-multimodel-system
    app.kubernetes.io/component: vertical-autoscaler
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-multimodel-api
  
  updatePolicy:
    updateMode: "Off"  # Recommendations only, manual application
  
  resourcePolicy:
    containerPolicies:
    - containerName: ai-multimodel-api
      minAllowed:
        cpu: "500m"
        memory: "1Gi"
      maxAllowed:
        cpu: "4"
        memory: "8Gi"
      controlledResources: ["cpu", "memory"]
      
    - containerName: prometheus-exporter
      minAllowed:
        cpu: "50m"
        memory: "64Mi"
      maxAllowed:
        cpu: "200m"
        memory: "256Mi"
      controlledResources: ["cpu", "memory"]

---
# üéØ Pod Disruption Budget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ai-multimodel-api-pdb
  namespace: production
  labels:
    app: ai-multimodel-api
    tier: backend
    environment: production
    app.kubernetes.io/name: ai-multimodel-system
    app.kubernetes.io/component: disruption-budget
spec:
  minAvailable: 3  # Always maintain at least 3 running pods
  selector:
    matchLabels:
      app: ai-multimodel-api
      tier: backend
      environment: production

---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ai-multimodel-frontend-pdb
  namespace: production
  labels:
    app: ai-multimodel-frontend
    tier: frontend
    environment: production
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: ai-multimodel-frontend
      tier: frontend
      environment: production